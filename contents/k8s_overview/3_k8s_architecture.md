Before we head into setting up a Kubernetes cluster, it is important to understand some of the basic

concepts.

This is to make sense of the terms that we will come across while setting up a Kubernetes cluster.

Let us start with nodes.

A node is a machine, physical or virtual, on which Kubernetes is installed.

A node is a worker machine and that is where containers will be launched by Kubernetes.

It was also known as minions in the past.

So you might hear these terms used interchangeably.

But what if the node on which your application is running fails?

Well, obviously our application goes down, so you need to have more than one nodes.

A cluster is a set of nodes grouped together.

This way, even if one node fails, you have your application still accessible from the other nodes.

Moreover, having multiple nodes helps in sharing load as well.

Now we have a cluster.

But who is responsible for managing the cluster?

Where is the information about the members of the cluster stored?

How are the nodes monitored?

When a node fails, how do you move the workload of the failed node to another worker node?

That's where the master comes in.

The master is another node with Kubernetes installed in it and is configured as a master.

The master watches over the nodes in the cluster and is responsible for the actual orchestration of

containers on the worker nodes.

When you install Kubernetes on a system, you're actually installing the following components an API

server and etcd service, a Kubernetes service, a container, runtime controllers and schedulers.

The API server acts as the front end for Kubernetes.

The user's management devices, command line interfaces all talk to the API server to interact with

the Kubernetes cluster.

Next is the etcd keystore.

Etcd is a distributed, reliable key value store used by Kubernetes to store all data used to manage

the cluster.

Think of it this way when you have multiple nodes and multiple masters in your cluster, etcd stores

all that information on all the nodes in the cluster in a distributed manner.

Etcd is responsible for implementing locks within the cluster to ensure that there are no conflicts

between the masters, the scheduler is responsible for distributing work or containers across multiple

nodes.

It looks for newly created containers and assigns them to nodes.

The controllers are the brain behind orchestration.

They're responsible for noticing and responding when nodes, containers, or endpoints goes down, the

controllers make decisions to bring up new containers.

In such cases, the container runtime is the underlying software that is used to run containers.

In our case, it happens to be Docker, but there are other options as well.

And finally, Kubelet is the agent that runs on each node in the cluster.

The agent is responsible for making sure that the containers are running on the nodes as expected.

So far we saw two types of servers master and worker, and a set of components that make up Kubernetes.

But how are these components distributed across different types of servers?

In other words, how does one server become a master and the other the slave?

The worker node, or minion as it is also known, is where the containers are hosted.

For example, Docker containers.

And to run Docker containers on a system, we need container runtime installed.

And that's where the container runtime falls.

In this case it happens to be Docker.

This doesn't have to be Docker.

There are other container runtime alternatives available, such as Rocket or Cryo.

But throughout this course we are going to use Docker as our container runtime engine.

The master server has the kube API server and that is what makes it a master.

Similarly, the worker nodes have the Kubelet agent that is responsible for interacting with the master

to provide health information of the worker node and carry out actions requested by the master on the

worker nodes.

All the information gathered are stored in a key value store on the master.

The key value store is based on the popular etcd framework.

As we just discussed.

The master also has the control manager and the scheduler.

There are other components as well, but we will stop there for now.

The reason we went through this is to understand what components constitute the master and worker nodes.

This will help us install and configure the right components on different systems.

When we set up our infrastructure and finally, we also need to learn a little bit about one of the

command line utilities known as the cube command line tool or cube CTL or cube control, as it is also

called the cube control tool, is used to deploy and manage applications on a Kubernetes cluster to

get cluster information, to get the status of other nodes in the cluster, and to manage many other

things.

The cube CTL run command is used to deploy an application on the cluster.

The cube CTL cluster info command is used to view information about the cluster and the cube CTL get

nodes command is used to list all the nodes part of the cluster.

That's all we need to know for now and we will keep learning more commands throughout this course.

We will explore more commands with cube CTL when we learn the associated concepts.

For now, just remember the run cluster info and get nodes commands and that will help us get through

the first few labs.

That's it for this lecture.

I will see you in the next lecture.