
So that's what I'm going to explain in this video.

So let's go back in time to the beginning of the container era.

And in the beginning there was just Docker and there were other tools like rocket.

But Docker's user experience made working with containers super simple, and hence Docker became the

most dominant container tool.

And then came Kubernetes to orchestrate Docker.

So Kubernetes was built to orchestrate Docker specifically in the beginning.

So Docker and Kubernetes were tightly coupled and back then Kubernetes only worked with Docker and didn't

support any other container solutions.

And then Kubernetes grew in popularity as a container orchestrator.

And now other container runtimes like rocket wanted in Kubernetes.

Users needed it to work with container runtimes that are other than just Docker.

And so Kubernetes introduced an interface called Container Runtime Interface or CRE.

So CRE allowed any vendor to work as a container runtime for Kubernetes as long as they adhere to the

OCI standards.

So OCI stands for Open Container Initiative, and it consists of an image spec and a runtime spec.

Image spec means the specifications on how an image should be built.

So that's what it defined.

An image spec defined the specifications on how an image should be built, and the runtime spec defined

the standards on how any container runtime should be developed.

So keeping these standards in mind, Anyone can build a container runtime that can be used by anybody

to work with Kubernetes.

So that was the idea.

So rocket and other container runtimes that adhere to the OCI standards were now supported as container

runtimes for Kubernetes via the CRI.

However, Docker wasn't built to support the CRI standards because remember, Docker was built way before

CRI was introduced and Docker still was the dominant container tool used by most.

Kubernetes had to continue to support Docker as well.

And so Kubernetes introduced what is known as Docker shim, which was a hacky but temporary way to continue

to support Docker outside of the container runtime interface.

While most other container runtimes worked through the CRI, Docker continued to work without it.

Now you see Docker isn't just a container runtime alone.

Docker consists of multiple tools that are put together, for example, the Docker CLI, the Docker

API, the build tools that help in building images.

There was support for volumes of security.

And finally also the container runtime called Runcie, the daemon that managed Runcie.

And that's that was called as container D.

So container D is CLI compatible and can work directly with Kubernetes as all other runtimes.

So container D can be used as a runtime on its own separate from Docker.

So now you have container D as a separate runtime and Docker separately.

So Kubernetes continued to maintain support for Docker Engine directly.

However, having to maintain the Docker shim was an unnecessary effort and added complications, so

it was decided in version 1.24 release of Kubernetes to remove Dockershim completely, and so support

for Docker was removed.

But you see all the images that were built before Docker was removed.

So all the Docker images continue to work because Docker followed the image spec from the OCI standard.

So all the images built by Docker follow the standard.

So they continue to work with Containerd.

But Docker itself was removed as a supported runtime from Kubernetes.

So that's the whole story.

And now let's look into container D more specifically.

So container D although is part of Docker, is a separate project on its own now and is a member of

CNCF with the graduated status.

So you can now install container D on its own without having to install Docker itself.

So if you don't really need Docker's other features, you could ideally just install container D alone.

So typically we ran containers using the docker run command when we had Docker.

And if Docker isn't installed then how do you run containers with just To container D.

Now once you install container D, it comes with a command line tool called Ktor.

And this tool is solely made for debugging container D and is not very user friendly as it only supports

a limited set of features.

And this is what you can see in the documentation pages for this particular tool.

So for the other than the limited set of features that it provides, anything any other way that you

want to interact with container D, you'll have to rely on making API calls directly, which is not

the the most user friendly way for us to operate.

So just to give you an idea, this can be the Ktor command can be used to perform basic container related

activities such as pull images.

For example, to pull uh, redis image, you will run the ktor images pull command followed by the address

of the image.

And to run a container we use the CTL run command and specify the image address.

But as I mentioned, this tool is solely made for debugging container D and is not very user friendly

and not to be used for running or managing containers on a production environment, so a better alternative

recommended is the nerd control tool or nerd CTL tool.

So the nerd control tool is a command line tool that's very similar to Docker.

So it's like Docker like CLI for container D.

It supports all or most of the options that Docker supports.

And apart from that it has the added benefit that it can give us access to the newest features implemented

into container D.

For example, we can work with the encrypted container images or other new feature that will eventually

be implemented into the Docker commands in the future.

It also supports lazy pulling of images, P2P image distribution, image signing and verifying and namespaces

in Kubernetes, which is not available in Docker.

So the nerd control tool works very similar to Docker CLI.

So instead of Docker, you would ideally simply have to replace it with node control.

So you can run almost all Docker commands that interact with containers like this.

So some examples are instead of running the docker run command to create a container to run a container,

you could just use the inert control run command.

And similarly, let's say you want to use some options for port mappings or exposing ports with the

Dash P option for the docker run command, you could do the same with node control.

Simply replace Docker with node control.

So that's pretty easy and straightforward.

So now that we have talked about CTR and the node control tool, it's important to talk about another

command line utility known as CRI, CTL or CRI control.

So earlier we talked about the container Runtime interface or CRI which is a single interface used to

connect CRI compatible container runtimes the container, the rocket and others.

The CRI control is a command line utility that is used to interact with the CRI compatible container

runtime.

So this is kind of an interaction from the Kubernetes perspective.

So this tool is maintained by developed and maintained by the Kubernetes community.

And this is this tool works across all the different container runtimes as opposed to earlier, you

had the Ctor and the node control.

Tool that were built by the container community specifically for container D.

This particular tool is from the Kubernetes perspective that works across.

Different container runtimes.

So it must be installed separately.

And it is used to.

Inspect and debug container runtime.

So this again is not ideally used to create containers.

Unlike Docker or the node control utility this is again a debugging tool.

You can technically create containers with the CRI control utility, but it's not easy.

It's only to be used for some special debugging purposes.

And the.

Remember that it works along with the Kubelet, so we know that the Kubelet is responsible for ensuring

that the specific number of containers or pods are available on on node at a time.

So if you go through the CRI control utility and try and create containers with it, then eventually

Kubelet is going to delete them because the Kubelet is unaware of some of those containers or pods that

are created outside of its knowledge.

So anything that it sees is going to go and delete it.

So because of those things, remember that the cry control utility is only used for debugging purposes

and getting into containers and all of that.

So let's look at some of the command line examples.

So you simply run the right control CRE CTL command for this.

And this can be used to perform basic container related activities such as pull images or list existing

images.

List containers very similar to the docker command where you use the PS command in docker.

You will run the PS command.

Here you would run the CRI control PS command and to run a command inside a container in Docker.

Remember we use the exact command and it's the same here along with the same options such as the dash,

I and T, and then you specify the container id and then the command that needs to be run to view the

logs.

Again you use the CRI control logs command very similar to Docker command.

And one major difference is that the CRI cry.

Control command is also aware of pods, so you can list pods by running the control pods command.

So this wasn't something that Docker was aware of.

So while working with Kubernetes in the past, we used Docker commands a lot to troubleshoot containers

and view logs, especially on the worker nodes.

Now you're going to use the cry control command to do so.

The syntax is a lot similar.

So it shouldn't be shouldn't be really hard.

So here's a chart that lists the comparison between Docker and the cry control command line tool.

So as you can see, a lot of commands such as attach exec images, info, inspect logs, PS stats,

version, etc. work exactly the same way and some of the commands to create, remove and start and stop

images works similarly too.

So a full list of differences can be found in the link given below.

Since, as I mentioned, cry control can be used to connect to any cry compatible runtime.

Remember to set the right endpoint if you have multiple container runtimes configured, or if you want

CRI control to interact with the specific runtime.

For example, if you haven't configured anything by default, it's going to connect to these sockets

in this particular order.

So it's going to try and connect to Dockershim first and then Containerd and then Cri-o.

And then you have the cri docker db.

That's the order that it follows.

But if you want to override that and set a specific endpoint, you use the runtime endpoint option with

the CRI control command line.

Or you could use the container runtime endpoint environment variable.

Set the environment variable to the right endpoint.

So to summarize, we have the Ktor command line utility that comes with Containerd and works with Containerd,

which is used for debugging purposes only and has a very limited set of features.

So ideally you wouldn't be using this at all.

So you can ignore this.

Then we have the nerd control CLI, which is again from the Containerd community.

But this is a docker like CLI for container D, used for general purpose to create containers and supports

the same or more features than docker CLI.

So this is something that I think we'll be using a lot more going forward.

And then we have the CRI control utility which is from the Kubernetes community, the mainly used to

interact with CRI compatible runtime.

So it's not just for container D.

This can be used for all CRI supported runtimes.

Again this is mainly for to be used for debugging purposes.

So if you look at the comparisons here you can see that CTR and CRI control are used mainly for debugging

purposes, whereas the control is used for general purpose.

The CTR and control are from the container the community and works with container D, whereas CRI control

is from the Kubernetes community and works across all CRI compatible runtimes.

So our labs originally had Docker installed on all the nodes.

So we used the Docker commands to troubleshoot.

But now it's all containerd.

So remember to use the the CRI control command instead to troubleshoot.

That's all for now.

Thank you for listening.